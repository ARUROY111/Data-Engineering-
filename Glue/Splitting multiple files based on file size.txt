import boto3
import pandas as pd
import urllib
import json
import io
import os

from awsglue.context import GlueContext
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from pyspark.sql import SparkSession

s3_client = boto3.client('s3')

# Getting the source bucket and key from the event
source_bucket = 'inp-task2'
source_key = 'Random.csv'

# print("source_bucket =", source_bucket)
# print("source_key =", source_key)

obj = s3_client.get_object(Bucket=source_bucket, Key=source_key)
file_stream = obj["Body"]
df = pd.read_csv(io.BytesIO(obj['Body'].read()), header=None)

response = s3_client.head_object(Bucket=source_bucket, Key=source_key)
size = response['ContentLength']
print(size)
# record_size = 4
if (size < 10000):
    record_size = 10
elif (size >= 10000 and size < 20000):
    record_size = 30
elif (size >= 20000 and size < 50000):
    record_size = 50
elif (size >= 50000 and size < 80000):
    record_size = 75
elif (size >= 80000 and size < 100000):
    record_size = 90
else:
    record_size = 100

slabs = [df[i:i + record_size] for i in range(0, df.shape[0], record_size)]
print(type(slabs))
Dest_bucket = 'opt-task2'

for i, chunk in enumerate(slabs):
    chunk.to_csv("/tmp/temp.csv", index=False)
    s3_client.upload_file("/tmp/temp.csv", Dest_bucket, 'output/' + f"chunk_{i}.csv")