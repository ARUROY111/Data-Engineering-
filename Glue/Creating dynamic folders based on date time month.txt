import boto3
import pandas as pd
import urllib
import json
import io
import os
import datetime


from awsglue.context import GlueContext
from awsglue.utils import getResolvedOptions
from pyspark.context import SparkContext
from pyspark.sql import SparkSession

s3 = boto3.resource('s3')

source_bucket = 'inp-task1'
source_key = 'Topic/boh/mika.csv'
keey = 'Topic/boh/mika.csv'
#print("source_bucket =", source_bucket)
#print("source_key =", source_key)
DST_BKT = 'opt-bkt'
        
#Creation of the destination folder name based on the current time
        
current_time = datetime.datetime.now()
folder_name = current_time.strftime('%Y')
folder_name1 = current_time.strftime('%m') 
folder_name2 = current_time.strftime('%d') 
folder_name3 = current_time.strftime('%H')
        
        
slicee = keey.split("/")
object_name = slicee[2]
Folder2_name = slicee[1]
Folder1_name = slicee[0]
        
        
#folder_name0 = "folder_name/folder_name1/folder_name2/folder_name3"
        
#destination_key1 = '{}/{}'.format(folder_name)
#destination_key2 = '{}/{}'.format(folder_name1) 
#destination_key3 = '{}/{}'.format(folder_name2)
        
destination_key = '{}/{}/{}/{}/{}/{}/{}'.format(Folder1_name,Folder2_name,folder_name,folder_name1,folder_name2,folder_name3 , object_name)
        
        
#destination_key = '{}/{}/{}/{}/{}/{}/{}/{}'.format(folder_target,folder_topic,folder_boh,folder_name,folder_name1,folder_name2,folder_name3 , source_key)
        
#desti_key ='{}/{}/{}/{}'.format(folder_target,folder_topic,folder_boh)
        
# Copying the object to the destination bucket
        
#print("current_time = ",current_time)
#print("folder_name = ",folder_name)
#print("destination_key =",destination_key)
        
s3.meta.client.copy_object(Bucket = DST_BKT,
CopySource={'Bucket': source_bucket, 
'Key': source_key}, Key= destination_key)